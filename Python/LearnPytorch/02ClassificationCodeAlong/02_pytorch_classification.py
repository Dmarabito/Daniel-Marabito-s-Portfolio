# -*- coding: utf-8 -*-
"""02_pytorch_Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L8L7-4VzP-isEf27yo7QeE-xs9k1VtA9
"""

import sklearn
from sklearn.datasets import make_circles
import torch
n_samples = 1000
x, y = make_circles(n_samples, noise=0.03,random_state=42)

print(f"length of x: {len(x)}")
print(f"length of y: {len(y)}")
print(f"First 5 of x: \n{x[:5]}")
print(f"First 5 of y: \n{y[:5]}")

import pandas as pd
circles = pd.DataFrame({"x1": x[:,0],
                                          "x2":x[:,1],
                                          "label":y})
circles.head(10)

import matplotlib.pyplot as plt
plt.scatter(x=x[:,0],
                  y=x[:,1],
                  c=y,
                  cmap=plt.cm.RdYlBu);

print(x.shape)
print(y.shape)

x_sample = x[0]
y_sample = y[0]
print(f"x sample value \n{x_sample}")

print(f"y sample value \n{y_sample}")

print(f"x sample shape value \n{x_sample.shape}")
print(f"y sample shape value \n {y_sample.shape}")

x = torch.from_numpy(x).type(torch.float)
y = torch.from_numpy(y).type(torch.float)

print(x[:5])
print(y[:5])

# Split data into training and test sets
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,
                                                                                y,
                                                                               test_size=0.2,
                                                                               random_state=42)

print(len(x_train))
    print(len(x_test))

    print(len(y_train))
    print(len(y_test))

import torch
from torch import nn
device = "cuda" if torch.cuda.is_available() else "cpu"

class CircleModelV0 (nn.Module):
  def __init__(self):
    super().__init__()
    self.layer_1 = nn.Linear(in_features = 2, out_features=5)
    self.layer_2 = nn.Linear(in_features = 5, out_features=1)

  def forward(self, x):
    return self.layer_2(self.layer_1(x))

model_0 = CircleModelV0().to(device)
print(model_0)

model_0 = nn.Sequential(
    nn.Linear(in_features=2, out_features=5),
    nn.Linear(in_features=5,out_features=1)
).to(device=device)

print(model_0.state_dict)

with torch.inference_mode():
  untrained_preds = model_0(x_test.to(device))
  print(f"length of predictions; {len(untrained_preds)}, Shape: {untrained_preds.shape}")
  print(f"length of test samples: {len(x_test)}, Shape: {x_test.shape}")
  print(f"first 10 predictions {untrained_preds[:10]}")
  print(f"first 10 labels {y_test[:10]}")

"""*************************************************************************************************************************"""

loss_fn = nn.BCEWithLogitsLoss()
optimizer = torch.optim.SGD(params=model_0.parameters(), lr =0.1)

def accuracy_fn(y_true,y_pred):
  correct = torch.eq(y_true,y_pred).sum().item()
  acc = correct/len(y_pred)*100
  return acc

model_0.eval()
with torch.inference_mode():
  y_logits = model_0(x_train.to(device))[:5]
print(y_logits)

y_pred_probs = torch.sigmoid(y_logits)
print(y_pred_probs)
print(torch.round(y_pred_probs))

y_preds = torch.round(y_pred_probs)

y_pred_labels = torch.round(torch.sigmoid(model_0(x_train.to(device)[:5])))
print(torch.eq(y_preds.squeeze(),y_pred_labels.squeeze()))

print(y_preds.squeeze())

torch.manual_seed(42)
torch.cuda.manual_seed(42)
x_train = x_train.to(device=device)
y_train = y_train.to(device=device)
x_test = x_test.to(device=device)
y_test=y_test.to(device=device)
#model_0.to(device=device)
epochs = 100
for epoch in range(epochs):
  model_0.train()
  y_logits = model_0(x_train).squeeze()
  y_pred = torch.round(torch.sigmoid(y_logits))
  loss = loss_fn(y_logits, y_train)
  acc = accuracy_fn(y_true=y_train,y_pred=y_pred)
  optimizer.zero_grad
  loss.backward()
  optimizer.step()

  model_0.eval()
  with torch.inference_mode():
    test_logits = model_0(x_test).squeeze()
    test_pred = torch.round(torch.sigmoid(test_logits)).squeeze()
    test_loss = loss_fn(test_logits, y_test)
    test_acc = accuracy_fn(y_true=y_test , y_pred=test_pred)
  if epoch % 10 == 0:
    print(f"epoch: {epoch}")
    print(f"loss : {loss:.5f}, test loss: {test_loss:.5f}")
    print(f"accuracy: {acc:.2f}, test accuracy: {test_acc:.2f}")

import requests
from pathlib import Path

if (Path("helper_functions.py").is_file()):
  print("helper_functions.py already exists")
else:
  print("Downloading helper_functions.py")
  request = requests.get("https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/refs/heads/main/helper_functions.py")
  with open("helper_functions.py", "wb") as f:
    f.write(request.content)
  from helper_functions import plot_predictions, plot_decision_boundary

#from helper_functions import plot_decision_boundary
plt.figure(figsize=(12,6))
plt.subplot(1 , 2 , 1)
plt.title("train")

plot_decision_boundary(model_0, x_train, y_train)
plt.subplot(1, 2, 2)
plt.title("test")
plot_decision_boundary(model_0, x_test, y_test)

class CircleModelV1(nn.Module):
  def __init__(self):
    super().__init__()
    self.layer_1 = nn.Linear(in_features=2, out_features=10)
    self.layer_2 = nn.Linear(in_features=10, out_features=10)
    self.layer_3 = nn.Linear(in_features=10, out_features=1)

  def forward(self, x):
    #z = self.layer_1(x)
    #z= self.layer_2(z)
    #z=self.layer_3(z)
    #return z
    return self.layer_3(self.layer_2(self.layer_1(x)))

model_1 = CircleModelV1().to(device=device)
print(model_1)

loss_fn = nn.BCEWithLogitsLoss()
optimizer = torch.optim.SGD(params=model_1.parameters(), lr=0.1)

torch.manual_seed(42)
torch.cuda.manual_seed(42)

epochs = 1000

x_train, y_train = x_train.to(device), y_train.to(device)
x_test, y_test = x_test.to(device), y_test.to(device)

for epoch in range(epochs):
  model_1.train()
  y_logits = model_1(x_train).squeeze()
  y_preds = torch.round(torch.sigmoid(y_logits))
  loss = loss_fn(y_logits, y_train)
  acc = accuracy_fn(y_true=y_train,y_pred=y_preds)
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()

  model_1.eval()
  with torch.inference_mode():
    test_logits = model_1(x_test).squeeze()
    test_preds = torch.round(torch.sigmoid(test_logits))
    test_loss = loss_fn(test_logits, y_test)
    test_acc = accuracy_fn(y_true=y_test,y_pred=test_preds)
  if (epoch%100 == 0):
    print(f"epoch: {epoch}/ {epochs}, progress = {(epoch/epochs)*100:.2f}%")
    print(f"training loss: {loss:.5f}, training accuracy {acc:.2f}%")
    print(f"test loss: {test_loss:.2f}, test accuracy {test_acc:.2f}%")
    print("")

#from helper_functions import plot_decision_boundary
plt.figure(figsize=(12,6))
plt.subplot(1 , 2 , 1)
plt.title("train")

plot_decision_boundary(model_1, x_train, y_train)
plt.subplot(1, 2, 2)
plt.title("test")
plot_decision_boundary(model_1, x_test, y_test)

weight = 0.7
bias = 0.3
start = 0
end = 1
step = 0.01
x_regression = torch.arange(start=start,end=end,step=step).unsqueeze(dim=1)
y_regression = weight*x_regression + bias

print(len(x_regression))
print(x_regression[:5])
print(y_regression[:5])

train_split = int(0.8*len(x_regression))
x_regression_train = x_regression[:train_split]
x_regression_test = x_regression[train_split:]
y_regression_train = y_regression[:train_split]
y_regression_test = y_regression[train_split:]
print(len(x_regression_train))
print(len(x_regression_test))
print(len(y_regression_train))
print(len(y_regression_test))

plot_predictions(train_data=x_regression_train,
                 train_labels=y_regression_train,
                 test_data=x_regression_test,
                 test_labels=y_regression_test)

model_2 = nn.Sequential(
    nn.Linear(in_features=1, out_features=10),
    nn.Linear(in_features=10, out_features=10),
    nn.Linear(in_features=10, out_features=1)
).to(device)
print(model_2)

loss_fn = nn.L1Loss()
optimizer = torch.optim.SGD(params= model_2.parameters(),lr=0.01)

torch.manual_seed(42)
torch.cuda.manual_seed(42)
epochs = 10000
x_regression_train = x_regression_train.to(device)
y_regression_train = y_regression_train.to(device)
x_regression_test = x_regression_test.to(device)
y_regression_test = y_regression_test.to(device)
for epoch in range(epochs):
  model_2.train()
  y_pred = model_2(x_regression_train)
  loss = loss_fn(y_pred, y_regression_train)
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()

  model_2.eval()
  with torch.inference_mode():
    test_pred = model_2(x_regression_test)
    test_loss = loss_fn(test_pred, y_regression_test)
    if (epoch%100==0):
      print(f"epoch: {epoch}")
      print(f"training loss: {loss:5f}")
      print(f"test loss: {test_loss:5f}")

model_2.eval()
with torch.inference_mode():
  y_preds = model_2(x_regression_test)
plot_predictions(train_data=x_regression_train.cpu(), train_labels=y_regression_train.cpu(), test_data=x_regression_test.cpu(), test_labels=y_regression_test.cpu(), predictions=y_preds.cpu())

import matplotlib.pyplot as plt
from sklearn.datasets import make_circles
n_samples=1000
x, y = make_circles(n_samples=n_samples, noise=0.03,random_state=42)
plt.scatter(x[:,0],x[:,1],c=y, cmap=plt.cm.RdYlBu)

import torch
from sklearn.model_selection import train_test_split
x = torch.from_numpy(x).type(torch.float)
y = torch.from_numpy(y).type(torch.float)
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
print(x_train[:5])
print(y_train[:5])

from torch import nn
class CircleModelV2(nn.Module):
  def __init__(self):
    super().__init__()
    self.layer_1 = nn.Linear(in_features=2, out_features=10)
    self.layer_2 = nn.Linear(in_features=10, out_features=10)
    self.layer_3 = nn.Linear(in_features=10, out_features=1)
    self.relu = nn.ReLU()

  def forward(self, x):
   return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))

model_3 = CircleModelV2().to(device=device)
print(model_3)

loss_fn = nn.BCEWithLogitsLoss()
optimizer = torch.optim.SGD(params=model_3.parameters(), lr=0.1)

torch.manual_seed(42)
torch.cuda.manual_seed(42)

x_train, y_train = x_train.to(device), y_train.to(device)
x_test, y_test = x_test.to(device), y_test.to(device)
epochs = 10000
for epoch in range(epochs):
  model_3.train()
  y_logits = model_3(x_train).squeeze()
  y_pred = torch.round(torch.sigmoid(y_logits))
  loss = loss_fn(y_logits, y_train)
  acc = accuracy_fn(y_true=y_train, y_pred=y_pred)
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()
  model_3.eval()
  with torch.inference_mode():
    test_logits = model_3(x_test).squeeze()
    test_preds = torch.round(torch.sigmoid(test_logits))
    test_loss = loss_fn(test_logits, y_test)
    test_acc = accuracy_fn(y_true=y_test, y_pred=test_preds)
    if (epoch % 500 == 0):
      print(f"epoch: {epoch}")
      print(f"training loss: {loss:.4f}, training accuarcy: {acc:.2f}%")
      print(f"test loss: {test_loss:.4f}, test accuracy: {test_acc:.2f}%")

model_3.eval()
with torch.inference_mode():
  y_preds = torch.round(torch.sigmoid(model_3(x_test))).squeeze()
print(y_preds[:10])
print(y_test[:10])

plt.figure(figsize=(12,6))
plt.subplot (1,2,1)
plt.title("Train")
plot_decision_boundary(model_1, x_train, y_train)
plt.subplot(1,2,2)
plt.title("test")
plot_decision_boundary(model_1, x_test, y_test)

plt.figure(figsize=(12,6))
plt.subplot (1,2,1)
plt.title("Train")
plot_decision_boundary(model_3, x_train, y_train)
plt.subplot(1,2,2)
plt.title("test")
plot_decision_boundary(model_3, x_test, y_test)

A = torch.arange(start= -10, end= 10, step= 1, dtype=torch.float32)
print(A.dtype)

plt.plot(A)

plt.plot(torch.relu(A))

plt.plot(torch.maximum(A, torch.zeros(20)))

def relu(x:torch.Tensor) -> torch.Tensor:
  return torch.maximum(torch.tensor(0), x)

plt.plot(relu(A))

def sigmoid(x:torch.Tensor) -> torch.Tensor:
  return 1/(1+torch.exp(-x))

plt.plot(sigmoid(A))

#Creating our toy multi-class data set.
import torch
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs

NUM_CLASSES = 4
NUM_FEATURES = 2
RANDOM_SEED = 42

#Generate data
x_blob, y_blob = make_blobs(n_samples=1000,
                            n_features=NUM_FEATURES,
                            centers=NUM_CLASSES,
                            cluster_std=1.5,
                            random_state=RANDOM_SEED)

x_blob = torch.from_numpy(x_blob).type(torch.float)
y_blob = torch.from_numpy(y_blob).type(torch.LongTensor)

#split data into train/test
x_blob_train, x_blob_test, y_blob_train, y_blob_test = train_test_split(x_blob, y_blob, test_size=0.2, random_state=RANDOM_SEED)

#plot data
plt.figure(figsize=(10,7))

plt.scatter(x_blob[:, 0],x_blob[:, 1], c=y_blob, cmap=plt.cm.RdYlBu)

# building a multiclass classification model.
#device agnostic code
device = "cuda" if torch.cuda.is_available() else "cpu"
print(device)

class blobModel(nn.Module):
  def __init__(self, input_features, output_features, hidden_units=8):
    """The model for our multiclass classification problem.
    Args:
    input_features (int): number of input features for the model.
    output_features (int): number of output features for the model.
    hidden_units (int): number of hidden units between layers. defaults to 8.

    Returns:
    A module with the provided input, output features and hidden units.
    """
    super().__init__()
    self.linear_layer_stack = nn.Sequential(
        nn.Linear(in_features=input_features, out_features=hidden_units),
        nn.ReLU(),
        nn.Linear(in_features=hidden_units, out_features=hidden_units),
        nn.ReLU(),
        nn.Linear(in_features=hidden_units, out_features=output_features)
    )

  def forward(self, x):
    return self.linear_layer_stack(x)

model_4 = blobModel(input_features=2, output_features=4, hidden_units=8).to(device)
print(model_4)

loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(params=model_4.parameters(), lr=0.1)

model_4.eval()
with torch.inference_mode():
  y_logits = model_4(x_blob_test.to(device))
print(y_logits[:10])
print(y_blob_test[:10])

y_pred_probs = torch.softmax(input=y_logits, dim=1)
print(y_logits[:5])
print(y_pred_probs[:5])

print(torch.sum(y_pred_probs[0]))
print(torch.max(y_pred_probs[0]))
print(torch.argmax(y_pred_probs[0]))

y_preds = torch.argmax(y_pred_probs, dim=1)
print(y_preds[:5])
print(y_blob_test[:5])

torch.manual_seed(42)
torch.cuda.manual_seed(42)
x_blob_train, x_blob_test = x_blob_train.to(device), x_blob_test.to(device)
y_blob_train, y_blob_test = y_blob_train.to(device), y_blob_test.to(device)

epochs = 100

for epoch in range(epochs):
  model_4.train()

  y_logits = model_4(x_blob_train)
  y_pred_probs = torch.softmax(input=y_logits, dim=1)
  y_preds = y_pred_probs.argmax(dim=1)

  loss = loss_fn(y_logits, y_blob_train)
  acc = accuracy_fn(y_true=y_blob_train, y_pred=y_preds)

  optimizer.zero_grad()

  loss.backward()

  optimizer.step()
  model_4.eval()
  with torch.inference_mode():
    test_logits = model_4(x_blob_test)
    test_preds = torch.softmax(input=test_logits, dim=1).argmax(dim=1)
    test_loss = loss_fn(test_logits, y_blob_test)
    test_acc = accuracy_fn(y_true=y_blob_test, y_pred=test_preds)
  if epoch%10 == 0:
    print(f"epoch {epoch}")
    print(f"loss: {loss:.4f}, accuracy: {acc:.2f}")
    print(f"test loss {test_loss:.4f}, test accuracy {test_acc:.2f}")

model_4.eval()
with torch.inference_mode():
  y_logits = model_4(x_blob_test)
  print(y_logits[:10])

y_pred_probs = torch.softmax(y_logits, dim=1)
y_pred_labels = y_pred_probs.argmax(dim=1)
print(y_pred_labels)

print(y_blob_test[:10])

print(y_blob_test == y_pred_labels)

plt.figure(figsize=(12,6))
plt.subplot(1, 2 , 1)
plt.title("train")
plot_decision_boundary(model_4, x_blob_train, y_blob_train)
plt.subplot(1, 2 , 2)
plt.title("test")
plot_decision_boundary(model_4, x_blob_test, y_blob_test)

!pip install torchmetrics

from torchmetrics import Accuracy
torchmetric_accuarcy = Accuracy(task="multiclass", num_classes=4).to(device)
torchmetric_accuarcy(y_pred_labels, y_blob_test)